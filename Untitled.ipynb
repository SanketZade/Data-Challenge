{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd # CSV file I/O (pd.read_csv)\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words( headlines ):               \n",
    "    headlines_onlyletters = re.sub(\"[^a-zA-Z]\", \" \",headlines) #Remove everything other than letters     \n",
    "    words = headlines_onlyletters.lower().split() #Convert to lower case, split into individual words    \n",
    "    stops = set(stopwords.words(\"english\"))  #Convert the stopwords to a set for improvised performance                 \n",
    "    meaningful_words = [w for w in words if not w in stops]   #Removing stopwords\n",
    "    return( \" \".join( meaningful_words )) #Joining the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         There Were 2 Mass Shootings In Texas Last Week...\n",
       "1         Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2         Hugh Grant Marries For The First Time At Age 5...\n",
       "3         Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4         Julianna Margulies Uses Donald Trump Poop Bags...\n",
       "5         Morgan Freeman 'Devastated' That Sexual Harass...\n",
       "6         Donald Trump Is Lovin' New McDonald's Jingle I...\n",
       "7         What To Watch On Amazon Prime Thatâ€™s New Thi...\n",
       "8         Mike Myers Reveals He'd 'Like To' Do A Fourth ...\n",
       "9         What To Watch On Hulu Thatâ€™s New This WeekYo...\n",
       "10        Justin Timberlake Visits Texas School Shooting...\n",
       "11        South Korean President Meets North Korea's Kim...\n",
       "12        With Its Way Of Life At Risk, This Remote Oyst...\n",
       "13        Trump's Crackdown On Immigrant Parents Puts Mo...\n",
       "14        'Trump's Son Should Be Concerned': FBI Obtaine...\n",
       "15        Edward Snowden: There's No One Trump Loves Mor...\n",
       "16        Booyah: Obama Photographer Hilariously Trolls ...\n",
       "17        Ireland Votes To Repeal Abortion Amendment In ...\n",
       "18        Ryan Zinke Looks To Reel Back Some Critics Wit...\n",
       "19        Trump's Scottish Golf Resort Pays Women Signif...\n",
       "20        Weird Father's Day Gifts Your Dad Doesn't Know...\n",
       "21        Twitter #PutStarWarsInOtherFilms And It Was Un...\n",
       "22        Mystery 'Wolf-Like' Animal Reportedly Shot In ...\n",
       "23        North Korea Still Open To Talks After Trump Ca...\n",
       "24        2 Men Detonate Bomb Inside Indian Restaurant N...\n",
       "25        Thousands Travel Home To Ireland To Vote On Ab...\n",
       "26        Irish Voters Set To Liberalize Abortion Laws I...\n",
       "27        Warriors Coach Steve Kerr Calls NFL Ban On Pro...\n",
       "28        In Historic Victory, Barbados Elects First Fem...\n",
       "29        Police Killed At Least 378 Black Americans Fro...\n",
       "                                ...                        \n",
       "200823    Chris Gregoire, Washington State Governor, Dis...\n",
       "200824    Glenn Close On 'Albert Nobbs', Gender Bending ...\n",
       "200825    Tinker and Change the WorldTinkering -- that h...\n",
       "200826    Pregnant and Displaced: Double the DangerIt's ...\n",
       "200827    Tom Brady Helps Mentor, Tom Martinez, Find A K...\n",
       "200828    Boxer Puppy And Cows Make Friends During Walk ...\n",
       "200829    'Black Smoker' Vents: New Species Discovered N...\n",
       "200830    Green Activists: 50 And OlderIf you look at so...\n",
       "200831    Winter Weather Photo Contest: Submit Your Own ...\n",
       "200832    Insects Top Newly Discovered Species ListSpeci...\n",
       "200833    Four More Bank Closures Mark the Week of Janua...\n",
       "200834    Everything You Need To Know About Overdraft Fe...\n",
       "200835    Walmart Waving Goodbye To Some GreetersAfter 3...\n",
       "200836    At World Economic Forum, Fear of Global Contag...\n",
       "200837    Positive Customer Experience: What's the Retur...\n",
       "200838    Sundance, Ice-T, and Shades of the American Ra...\n",
       "200839    'Girl With the Dragon Tattoo' India Release Ca...\n",
       "200840    'Don't Think': A Look At The Chemical Brothers...\n",
       "200841    Matthew Marks Discusses His New LA GalleryWas ...\n",
       "200842    Allard Van Hoorn's 'Urban Songline' Explores R...\n",
       "200843    Good Games -- Is It possible?I don't think peo...\n",
       "200844    Google+ Now Open for Teens With Some Safeguard...\n",
       "200845    Web WarsThese \"Web Wars\" threaten to rage on f...\n",
       "200846    First White House Chief Technology Officer, An...\n",
       "200847    Watch The Top 9 YouTube Videos Of The WeekIf y...\n",
       "200848    RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n",
       "200849    Maria Sharapova Stunned By Victoria Azarenka I...\n",
       "200850    Giants Over Patriots, Jets Over Colts Among  M...\n",
       "200851    Aldon Smith Arrested: 49ers Linebacker Busted ...\n",
       "200852    Dwight Howard Rips Teammates After Magic Loss ...\n",
       "Name: content, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_json(\"dataset.json\")\n",
    "result = pd.read_json(\"datasetresult.json\")\n",
    "news['content'] = news[['headline', 'short_description']].apply(lambda x: ''.join(x), axis=1)\n",
    "result['content'] = result[['headline', 'short_description']].apply(lambda x: ''.join(x), axis=1)\n",
    "X_train=news['content']\n",
    "X_test=result['content']\n",
    "Y_train=news['category']\n",
    "X_train = np.array(X_train);\n",
    "X_test = np.array(X_test);\n",
    "Y_train = np.array(Y_train);\n",
    "cleanHeadlines_train = [] #To append processed headlines\n",
    "cleanHeadlines_test = [] #To append processed headlines\n",
    "number_reviews_train = len(X_train) #Calculating the number of reviews\n",
    "number_reviews_test = len(X_test) #Calculating the number of reviews\n",
    "news['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,number_reviews_train):\n",
    "    cleanHeadline = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    cleanHeadlines_train.append( cleanHeadline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,number_reviews_test):\n",
    "    cleanHeadline = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    cleanHeadlines_test.append( cleanHeadline )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = sklearn.feature_extraction.text.CountVectorizer(analyzer = \"word\",max_features = 500)\n",
    "bagOfWords_train = vectorize.fit_transform(cleanHeadlines_train)\n",
    "X_train = bagOfWords_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWords_test = vectorize.transform(cleanHeadlines_test)\n",
    "X_test = bagOfWords_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWords_test = vectorize.transform(cleanHeadlines_test)\n",
    "X_test = bagOfWords_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorize.get_feature_names()\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_Regression = LogisticRegression()\n",
    "logistic_Regression.fit(X_train,Y_train)\n",
    "Y_predict = logistic_Regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin=[]\n",
    "i=1\n",
    "for item in Y_predict:\n",
    "    fin.append({\n",
    "        'id': i,\n",
    "        'category':item\n",
    "    })\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "with open(\"submission.json\", \"w\") as outfile:\n",
    "    for f in fin:\n",
    "        json.dump(f, outfile)\n",
    "        outfile.write(',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = pd.read_json(\"submission.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'according',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'administration',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'air',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'among',\n",
       " 'another',\n",
       " 'anti',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'around',\n",
       " 'art',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'attack',\n",
       " 'attention',\n",
       " 'away',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'based',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'become',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'black',\n",
       " 'body',\n",
       " 'book',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'business',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'campaign',\n",
       " 'cancer',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'change',\n",
       " 'check',\n",
       " 'child',\n",
       " 'children',\n",
       " 'christmas',\n",
       " 'city',\n",
       " 'climate',\n",
       " 'clinton',\n",
       " 'college',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'common',\n",
       " 'community',\n",
       " 'company',\n",
       " 'congress',\n",
       " 'control',\n",
       " 'could',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'create',\n",
       " 'culture',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'decision',\n",
       " 'democratic',\n",
       " 'democrats',\n",
       " 'different',\n",
       " 'director',\n",
       " 'divorce',\n",
       " 'dog',\n",
       " 'donald',\n",
       " 'done',\n",
       " 'dress',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'education',\n",
       " 'election',\n",
       " 'end',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'experience',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'father',\n",
       " 'favorite',\n",
       " 'fear',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'fight',\n",
       " 'film',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'first',\n",
       " 'five',\n",
       " 'food',\n",
       " 'former',\n",
       " 'found',\n",
       " 'four',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gay',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giving',\n",
       " 'global',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'good',\n",
       " 'gop',\n",
       " 'got',\n",
       " 'government',\n",
       " 'great',\n",
       " 'group',\n",
       " 'guide',\n",
       " 'gun',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'health',\n",
       " 'healthy',\n",
       " 'heart',\n",
       " 'help',\n",
       " 'high',\n",
       " 'hillary',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'host',\n",
       " 'hot',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'huffpost',\n",
       " 'human',\n",
       " 'husband',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'important',\n",
       " 'inside',\n",
       " 'instagram',\n",
       " 'instead',\n",
       " 'issue',\n",
       " 'james',\n",
       " 'job',\n",
       " 'john',\n",
       " 'justice',\n",
       " 'keep',\n",
       " 'key',\n",
       " 'kids',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'known',\n",
       " 'last',\n",
       " 'late',\n",
       " 'latest',\n",
       " 'law',\n",
       " 'lead',\n",
       " 'leaders',\n",
       " 'learn',\n",
       " 'learned',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'less',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'life',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'made',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'marriage',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'media',\n",
       " 'meet',\n",
       " 'men',\n",
       " 'michael',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'move',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'music',\n",
       " 'must',\n",
       " 'name',\n",
       " 'nation',\n",
       " 'national',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'night',\n",
       " 'north',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'obama',\n",
       " 'office',\n",
       " 'often',\n",
       " 'old',\n",
       " 'one',\n",
       " 'online',\n",
       " 'open',\n",
       " 'order',\n",
       " 'others',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'past',\n",
       " 'paul',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'pinterest',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'point',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'power',\n",
       " 'powerful',\n",
       " 'president',\n",
       " 'presidential',\n",
       " 'pretty',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'public',\n",
       " 'put',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'race',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasons',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'recipes',\n",
       " 'red',\n",
       " 'relationship',\n",
       " 'remember',\n",
       " 'report',\n",
       " 'republican',\n",
       " 'republicans',\n",
       " 'research',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risk',\n",
       " 'role',\n",
       " 'room',\n",
       " 'run',\n",
       " 'running',\n",
       " 'russia',\n",
       " 'said',\n",
       " 'sanders',\n",
       " 'save',\n",
       " 'say',\n",
       " 'says',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'security',\n",
       " 'see',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'senate',\n",
       " 'sense',\n",
       " 'series',\n",
       " 'set',\n",
       " 'sex',\n",
       " 'sexual',\n",
       " 'share',\n",
       " 'shooting',\n",
       " 'short',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sleep',\n",
       " 'small',\n",
       " 'social',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'son',\n",
       " 'south',\n",
       " 'space',\n",
       " 'special',\n",
       " 'spring',\n",
       " 'st',\n",
       " 'stand',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'states',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'story',\n",
       " 'street',\n",
       " 'stress',\n",
       " 'student',\n",
       " 'students',\n",
       " 'study',\n",
       " 'style',\n",
       " 'summer',\n",
       " 'super',\n",
       " 'support',\n",
       " 'supreme',\n",
       " 'sure',\n",
       " 'system',\n",
       " 'take',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tax',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'th',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tips',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'took',\n",
       " 'top',\n",
       " 'travel',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'trump',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'united',\n",
       " 'university',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'video',\n",
       " 'violence',\n",
       " 'vote',\n",
       " 'wall',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'washington',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'wedding',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weight',\n",
       " 'well',\n",
       " 'went',\n",
       " 'whether',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'win',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'words',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worst',\n",
       " 'would',\n",
       " 'wrong',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-af3c81bb093e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stops' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
